{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mempersiapkan data latih dan data uji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# function untuk konversi dari ubyte menjadi file TXT\n",
    "def convert(img_file, label_file, txt_file, n_images):\n",
    "  print(\"\\nOpening binary pixels and labels files \")\n",
    "  lbl_f = open(label_file, \"rb\")   # labels (digits)\n",
    "  img_f = open(img_file, \"rb\")     # pixel values\n",
    "  print(\"Opening destination text file \")\n",
    "  txt_f = open(txt_file, \"w\")      # output to write to\n",
    "\n",
    "  print(\"Discarding binary pixel and label headers \")\n",
    "  img_f.read(16)   # discard header info\n",
    "  lbl_f.read(8)    # discard header info\n",
    "\n",
    "  print(\"\\nReading binary files, writing to text file \")\n",
    "  print(\"Format: 784 pixels then labels, tab delimited \")\n",
    "  for i in range(n_images):   # number requested \n",
    "    lbl = ord(lbl_f.read(1))  # Unicode, one byte\n",
    "    for j in range(784):  # get 784 pixel vals\n",
    "      val = ord(img_f.read(1))\n",
    "      txt_f.write(str(val) + \"\\t\") \n",
    "    txt_f.write(str(lbl) + \"\\n\")\n",
    "  img_f.close(); txt_f.close(); lbl_f.close()\n",
    "  print(\"\\nDone \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Opening binary pixels and labels files \n",
      "Opening destination text file \n",
      "Discarding binary pixel and label headers \n",
      "\n",
      "Reading binary files, writing to text file \n",
      "Format: 784 pixels then labels, tab delimited \n",
      "\n",
      "Done \n",
      "\n",
      "Opening binary pixels and labels files \n",
      "Opening destination text file \n",
      "Discarding binary pixel and label headers \n",
      "\n",
      "Reading binary files, writing to text file \n",
      "Format: 784 pixels then labels, tab delimited \n",
      "\n",
      "Done \n"
     ]
    }
   ],
   "source": [
    "convert(\"dataset/train-images.idx3-ubyte\", \"dataset/train-labels.idx1-ubyte\", \"dataset/trainSet.txt\", 60000)\n",
    "convert(\"dataset/t10k-images.idx3-ubyte\", \"dataset/t10k-labels.idx1-ubyte\", \"dataset/testSet.txt\", 10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Membuat Data Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "class MNIST_Dataset(torch.utils.data.Dataset):\n",
    "  # 784 tab-delim pixel values (0-255) then label (0-9)\n",
    "  def __init__(self, src_file, transform = None):\n",
    "    all_xy = np.loadtxt(src_file, usecols=range(785),\n",
    "      delimiter=\"\\t\", comments=\"#\", dtype=np.float32)\n",
    "\n",
    "    self.transform = transform\n",
    "    tmp_x = all_xy[:, 0:784]  # all rows, cols [0,783]\n",
    "    tmp_x /= 255\n",
    "    tmp_x = tmp_x.reshape(-1, 1, 28, 28)\n",
    "    tmp_y = all_xy[:, 784]\n",
    "\n",
    "    self.x_data = \\\n",
    "      torch.tensor(tmp_x, dtype=torch.float32)\n",
    "    self.y_data = \\\n",
    "      torch.tensor(tmp_y, dtype=torch.int64)\n",
    "     \n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.x_data)\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "    lbl = self.y_data[idx]  # no use labels\n",
    "    pixels = self.x_data[idx]\n",
    "\n",
    "    if self.transform:\n",
    "      pixels = self.transform(pixels) \n",
    "    return (pixels, lbl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "\n",
    "# compose = transforms.Compose([\n",
    "#     transforms.ColorJitter(brightness=0.5, contrast=0.2),\n",
    "#     transforms.Normalize(mean=(0.1307), std=(0.3079)),\n",
    "# ])\n",
    "\n",
    "train_ds = MNIST_Dataset('dataset/trainSet.txt', transform=None)\n",
    "generator1 = torch.Generator().manual_seed(42)\n",
    "train_data, val_data = torch.utils.data.random_split(train_ds, [0.9, 0.1], generator=generator1)\n",
    "\n",
    "val_loader = DataLoader(val_data, batch_size=32, shuffle=True)\n",
    "train_loader = DataLoader(train_data, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mendefinisikan Network CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv2d(1, 8, kernel_size=3, stride=1, padding=0), #[1, 28, 28] -> [8, 26, 26]\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(8),\n",
    "            nn.MaxPool2d(kernel_size = 3, stride = 2), #[8, 26, 26] -> [8, 12, 12] \n",
    "            nn.Dropout(0.25))\n",
    "\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Conv2d(8, 16, kernel_size=3, stride=1, padding=0), #[8, 12, 12] -> [16, 10, 10]\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2) #[16, 10, 10] -> [16, 4, 4]\n",
    "        )\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(256, 50),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(50, 10)\n",
    "        )\n",
    "\n",
    "        self.gradient = None\n",
    "        self.relu = nn.ReLU()\n",
    "        self.resize = transforms.Resize(28)\n",
    "        self.flatten = nn.Flatten()\n",
    "\n",
    "    def forward(self, x):\n",
    "       z = self.layer1(x)\n",
    "       z = self.layer2(z)\n",
    "       h = z.register_hook(self.hook_activation)\n",
    "       z = self.flatten(z)\n",
    "       z = self.classifier(z)\n",
    "       return z\n",
    "    \n",
    "    def get_activations(self, x):\n",
    "        with torch.no_grad():\n",
    "            x = self.layer1(x)\n",
    "            return self.layer2(x)\n",
    "        \n",
    "    def hook_activation(self, grad):\n",
    "        self.gradient = grad\n",
    "    \n",
    "modifiedNet = Net()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# early stopping callback\n",
    "class EarlyStopper:\n",
    "    def __init__(self, patience=1, min_delta=0):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.counter = 0\n",
    "        self.min_validation_loss = float('inf')\n",
    "\n",
    "    def early_stop(self, validation_loss):\n",
    "        if validation_loss < self.min_validation_loss:\n",
    "            self.min_validation_loss = validation_loss\n",
    "            self.counter = 0\n",
    "        elif validation_loss > (self.min_validation_loss + self.min_delta):\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                return True\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [1/10]: 100%|██████████| 1688/1688 [00:14<00:00, 112.95it/s, accuracy=0.635, loss=1.13]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 0.5951412884478874\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [2/10]: 100%|██████████| 1688/1688 [00:14<00:00, 114.61it/s, accuracy=0.864, loss=0.457]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 0.3750975030533811\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [3/10]: 100%|██████████| 1688/1688 [00:14<00:00, 113.98it/s, accuracy=0.905, loss=0.32] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 0.27601327412226734\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [4/10]: 100%|██████████| 1688/1688 [00:17<00:00, 97.23it/s, accuracy=0.925, loss=0.253] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 0.2357093594334227\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [5/10]: 100%|██████████| 1688/1688 [00:18<00:00, 89.43it/s, accuracy=0.935, loss=0.217] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 0.20286043305346307\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [6/10]: 100%|██████████| 1688/1688 [00:15<00:00, 108.37it/s, accuracy=0.942, loss=0.194]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 0.2037480056701981\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [7/10]: 100%|██████████| 1688/1688 [00:14<00:00, 114.37it/s, accuracy=0.947, loss=0.179]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 0.17806565303831023\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [8/10]: 100%|██████████| 1688/1688 [00:14<00:00, 114.90it/s, accuracy=0.95, loss=0.167] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 0.17146968486857542\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [9/10]: 100%|██████████| 1688/1688 [00:14<00:00, 116.47it/s, accuracy=0.953, loss=0.159]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 0.16252754884276618\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [10/10]: 100%|██████████| 1688/1688 [00:14<00:00, 114.60it/s, accuracy=0.956, loss=0.151]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 0.1613413555587226\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(modifiedNet.parameters(), lr=1e-4)\n",
    "early_stopper = EarlyStopper(patience=3, min_delta=0.05)\n",
    "\n",
    "epochs = 10\n",
    "for epoch in range(epochs):\n",
    "    running_loss, running_accuracy = 0.0, 0.0\n",
    "    loop = tqdm(train_loader)\n",
    "    for i, data in enumerate(loop, 0):\n",
    "        loop.set_description(f\"Epoch [{epoch+1}/{epochs}]\")\n",
    "        inputs, labels = data\n",
    "        inputs, labels = inputs, labels\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # forward + backward + optimize\n",
    "        outputs = modifiedNet(inputs)\n",
    "        \n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            batch_acc = torch.sum(torch.argmax(outputs, dim=1) == labels) / labels.shape[0]\n",
    "            running_accuracy += batch_acc.item()\n",
    "            running_loss += loss.item()\n",
    "        loop.set_postfix(loss = running_loss / (i+1), accuracy = (running_accuracy / (i+1)), refresh = True)\n",
    "    \n",
    "    running_val_loss = 0.0\n",
    "    val_loss = 0.0\n",
    "    for i, data in enumerate(val_loader):\n",
    "        inputs, labels = data\n",
    "        outputs = modifiedNet(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        running_val_loss += loss.item()\n",
    "        val_loss = running_val_loss / (i+1)\n",
    "    print(f\"Validation loss: {val_loss}\")\n",
    "    if early_stopper.early_stop(val_loss):\n",
    "        break\n",
    "    \n",
    "          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(modifiedNet.state_dict(), \"models/Model.pt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlops",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
